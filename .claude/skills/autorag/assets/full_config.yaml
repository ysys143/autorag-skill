# AutoRAG 전체 옵션 설정 템플릿
# 모든 노드 타입과 주요 모듈 포함
#
# 주의: 이 설정은 많은 조합을 평가하므로 실행 시간이 오래 걸립니다.
# 프로토타이핑 시에는 simple_config.yaml 사용을 권장합니다.
#
# 사전 요구사항:
# pip install "AutoRAG[all]"
# export OPENAI_API_KEY="your-api-key"

# 벡터DB 설정
vectordb:
  - name: default
    db_type: chroma
    client_type: persistent
    embedding_model: openai
    collection_name: default_collection
    path: ${PROJECT_DIR}/data/chroma

node_lines:
  # 쿼리 확장 노드 라인
  - node_line_name: pre_retrieve_node_line
    nodes:
      - node_type: query_expansion
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
          speed_threshold: 10
          top_k: 10
          retrieval_modules:
            - module_type: bm25
              bm25_tokenizer: [porter_stemmer]
            - module_type: vectordb
              vectordb: default
        modules:
          - module_type: pass_query_expansion    # 확장 없음
          - module_type: query_decompose         # 쿼리 분해
            generator_module_type: llama_index_llm
            llm: openai
            model: [gpt-4o-mini]
          - module_type: hyde                    # HyDE
            generator_module_type: llama_index_llm
            llm: openai
            model: [gpt-4o-mini]
            max_token: 64
          - module_type: multi_query_expansion   # 다중 쿼리
            generator_module_type: llama_index_llm
            llm: openai
            temperature: [0.5, 1.0]

  # 검색 노드 라인
  - node_line_name: retrieve_node_line
    nodes:
      # BM25 검색
      - node_type: lexical_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision,
                    retrieval_ndcg, retrieval_map, retrieval_mrr]
          speed_threshold: 10
        top_k: 10
        modules:
          - module_type: bm25
            bm25_tokenizer: [porter_stemmer, space]

      # 벡터 검색
      - node_type: semantic_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision,
                    retrieval_ndcg, retrieval_map, retrieval_mrr]
          speed_threshold: 10
        top_k: 10
        modules:
          - module_type: vectordb
            vectordb: default

      # 하이브리드 검색
      - node_type: hybrid_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision,
                    retrieval_ndcg, retrieval_map, retrieval_mrr]
          speed_threshold: 10
        top_k: 10
        modules:
          - module_type: hybrid_rrf
            weight_range: (4, 80)
          - module_type: hybrid_cc
            normalize_method: [mm, tmm, z, dbsf]
            weight_range: (0.0, 1.0)
            test_weight_size: 51

      # 문서 증강
      - node_type: passage_augmenter
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
          speed_threshold: 5
        top_k: 5
        embedding_model: openai
        modules:
          - module_type: pass_passage_augmenter
          - module_type: prev_next_augmenter
            mode: next

      # 문서 필터링
      - node_type: passage_filter
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
          speed_threshold: 5
        modules:
          - module_type: pass_passage_filter
          - module_type: similarity_threshold_cutoff
            threshold: 0.85
          - module_type: similarity_percentile_cutoff
            percentile: 0.6
          - module_type: threshold_cutoff
            threshold: 0.85
          - module_type: percentile_cutoff
            percentile: 0.6

      # 문서 압축
      - node_type: passage_compressor
        strategy:
          metrics: [retrieval_token_f1, retrieval_token_recall, retrieval_token_precision]
          speed_threshold: 10
        modules:
          - module_type: pass_compressor
          - module_type: tree_summarize
            llm: openai
            model: gpt-4o-mini
          - module_type: refine
            llm: openai
            model: gpt-4o-mini

  # 생성 노드 라인
  - node_line_name: post_retrieve_node_line
    nodes:
      # 프롬프트 구성
      - node_type: prompt_maker
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
            - metric_name: g_eval
          speed_threshold: 10
          generator_modules:
            - module_type: llama_index_llm
              llm: openai
              model: [gpt-4o-mini]
        modules:
          - module_type: fstring
            prompt:
              - "Question: {query}\n\nContext:\n{retrieved_contents}\n\nAnswer:"
              - "Based on the following context, answer the question.\n\nContext: {retrieved_contents}\n\nQuestion: {query}\n\nAnswer:"
          - module_type: long_context_reorder
            prompt: "Question: {query}\n\nContext:\n{retrieved_contents}\n\nAnswer:"
          - module_type: window_replacement
            prompt: "Question: {query}\n\nContext:\n{retrieved_contents}\n\nAnswer:"

      # 답변 생성
      - node_type: generator
        strategy:
          metrics:
            - metric_name: bleu
            - metric_name: meteor
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
            - metric_name: g_eval
          speed_threshold: 10
        modules:
          - module_type: llama_index_llm
            llm: [openai]
            model: [gpt-4o-mini, gpt-4o]
            temperature: [0.5, 1.0]
          - module_type: openai_llm
            llm: gpt-4o-mini
            temperature: 0.8
